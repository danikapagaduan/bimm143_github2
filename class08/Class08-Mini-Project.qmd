---
title: "Class 8: Machine Learning Mini Project"
author: "Danika"
format: pdf
---
In today's mini-project we will explore a complete analysis using unsupervised learning techniques covered in class (clustering and PCA for now).

The data itself comes from the Wisconsin Breast Cancer Diagnosis Data Set FNA breast biopsy data.

# Save your input data file into your Project directory
```{r}
fna.data <- "WisconsinCancer.csv"
```


# Complete the following code to input the data and store as wisc.df
```{r}
wisc.df <-read.csv(fna.data, row.names=1)
head(wisc.df)
```
Remove the diagnosis column and keep it in a separate vector for later
# We can use -1 here to remove the first column
```{r}
diagnosis <- as.factor(wisc.df[,1])
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

## Exploratory data analysis

The first step of any data analysis, unsupervised or supervised, is to familiarize yourself with the data.

> Q1. How many observations (patients) are in this dataset?

```{r}
nrow(wisc.data)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

First find the column names
```{r}
colnames(wisc.data)
```

Next I need to search within the column names for the "_mean" pattern. The 'grep()' function might help here.

```{r}
inds <- grep("_mean",colnames((wisc.data)))
length(inds)
```
> Q. How many dimensions are in this dataset?

```{r}
ncol(wisc.data)
```
# Principal Component Analysis

First do we need to scale the data before PCA or not.

# Check column means and standard deviations
```{r}
round(apply(wisc.data,2,sd),3)
```

Looks like we need to scale.

# Perform PCA on wisc.data by completing the following code
```{r}
wisc.pr <- prcomp( wisc.data, scale=TRUE)
summary(wisc.pr)
```


> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs capture 72%

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs capture 91%

# Interpreting PCA results

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

There is a distinct difference between 2 sets of data. We need to better organize the data to make sense of it.

## PC Plot

We need to make out plot of PC1 vs PC2 (aka score plot, PC-plot, etc.) The main results of PCA...

```{r}
biplot(wisc.pr)
```

# Scatter plot observations by components 1 and 2
```{r}
plot( wisc.pr$x[,1],wisc.pr$x[,2], col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```
> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

The red and the black localization of points are around the same areas on the graph. The majority of the red dots is on the left of the majority of the black dots.

# Repeat for components 1 and 3
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

```{r}
library(ggplot2)

pc <- as.data.frame(wisc.pr$x)
pc$diagnosis <- diagnosis

ggplot(pc)+
  aes(PC1, PC2, col=diagnosis)+
  geom_point()
```

## Variance Explained

Calculate the variance of each principal component by squaring the sdev component of wisc.pr (i.e. wisc.pr$sdev^2). Save the result as an object called pr.var.

# Calculate variance of each component
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.

# Variance explained by each principal component: pve
```{r}
pve <- pr.var / sum(pr.var)
```

# Plot variance explained for each principal component
```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

# Alternative scree plot of the same data, note data driven y-axis
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
# Communicating PCA results
> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

How much do the original variables contribute to the new PCs that we have calculated? To get at this data we can look at the '$rotation' component of the returned PCA object

```{r}
head(wisc.pr$rotation[,1:3])
```

Focus in on PC1
```{r}
wisc.pr$rotation["concave.points_mean",1]
```


There is complicated mix of variables that go together to make up PC1 i.e. there are many of the original variables that together contribute highly to PC1.
```{r}
loadings <- as.data.frame(wisc.pr$rotation)

ggplot(loadings)+
  aes(PC1, rownames(loadings))+
  geom_col()
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

5 PCs for 85%

# Hierarchical clustering

The goal of this section is to do hierarchical clustering of the original data.

First we will scale the data, then distance matrix, then hclust

```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(dist(scale(wisc.data)))
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

Height 19

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```


Cut this tree to yield cluster membership vector with 'cutree()' function. How well do the two clusters separate the M and B diagnoses?

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=19)
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

I think a better cluster vs diagnoses match would be at 10 since there is a bigger distinction between benign and malignant data points.

```{r}
x <- 10
wisc.hclust.clusters <- cutree(wisc.hclust, k=x)
table(wisc.hclust.clusters, diagnosis)
```


Try clustering in 3 PCs, that is PC1, PC2, and PC3 as input
```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
```
```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="complete")
plot(wisc.pr.hclust)
```

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="single")
plot(wisc.pr.hclust)
```

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="average")
plot(wisc.pr.hclust)
```

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning

My favorite method is the 'complete' because it distributes the branches more evenly and is easier to see the different branches.

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

The newly created model with the four cluster separated out the points visually with the plot in different colors, but the table does not make a clear distinction between the patients with malignant and benign conditions.

#Combine methods: PCA and HCLUST
My PCA results were interesting as they showed a separation of M and B samples along PC1.
 
Let's cut this tree into four groups/clusters

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps2 <- cutree(wisc.pr.hclust, k=4)
```

 
```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2], col=diagnosis)
```
 I want to cluster my PCA results - that is use 'wisc.pr$x' as input 'hclust()' 


```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps2)
```


And my tree result figure
```{r}
plot(wisc.pr.hclust)
```


# Compare to actual diagnoses
```{r}
table(grps2, diagnosis)
```

```{r}
(179+333)/nrow(wisc.data)
```















